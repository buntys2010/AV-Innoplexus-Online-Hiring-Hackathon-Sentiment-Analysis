{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_F3WbcTw.csv\")\n",
    "test = pd.read_csv(\"../input/test_tOlRoBf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing,...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before th...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients r...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no rel...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0  Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing,...   \n",
       "1  I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before th...   \n",
       "2  Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients r...   \n",
       "3                                                                             Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction   \n",
       "4  Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no rel...   \n",
       "\n",
       "         drug  sentiment  \n",
       "0     gilenya          2  \n",
       "1     gilenya          2  \n",
       "2  fingolimod          2  \n",
       "3     ocrevus          2  \n",
       "4     gilenya          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.724569\n",
       "1    0.158553\n",
       "0    0.116878\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'] + \"This observation is for \" + train['drug']\n",
    "test['text'] = test['text'] + \"This observation is for \" + test['drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    new_text =re.sub('[0-9]', '', text)\n",
    "    new_text = re.sub(r\"\\u200b\",\"\",new_text)\n",
    "    new_text = re.sub(r\"\\.+\",\".\",new_text)\n",
    "    new_text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',new_text, flags=re.MULTILINE)\n",
    "    new_text = re.sub(\"'\", \"\", new_text)\n",
    "    new_text = re.sub(r'↑', '', new_text)\n",
    "    new_text = re.sub(\"\\t\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\xa0\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\(|\\)|\\[|\\]\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\n\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\.\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\,\", \" \", new_text)\n",
    "    new_text = re.sub(\"[/%]\", \" \", new_text)\n",
    "    new_text = re.sub('[/%:;]', '', new_text)\n",
    "    new_text = re.sub(' +', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 6s, sys: 48 ms, total: 3min 6s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove URL's from train and test\n",
    "for index, row in train['text'].iteritems():\n",
    "    train['text'][index] = pre_process(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 24 ms, total: 1.85 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for index, row in test['text'].iteritems():\n",
    "    test['text'][index] = pre_process(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URL's from train and test\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "train['text'] = train['text'].str.replace(\"[0-9]\", \" \")\n",
    "test['text'] = test['text'].str.replace(\"[0-9]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:232: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Handle Won't \n",
    "\n",
    "replaceWords1 = { \"won't\":\"will not\",\"$&@*#\":\"in most profane vulgar shitty terms\",\"#$&@*#\":\"shitty\",\n",
    " \"can't\":\"cannot\",\"aren't\": 'are not',\n",
    " \"Aren't\": 'Are not',\n",
    " \"AREN'T\": 'ARE NOT',\n",
    " \"C'est\": \"C'est\",\n",
    " \"C'mon\": \"C'mon\",\n",
    " \"c'mon\": \"c'mon\",\n",
    " \"can't\": 'cannot',\n",
    " \"Can't\": 'Cannot',\n",
    " \"CAN'T\": 'CANNOT',\n",
    " \"con't\": 'continued',\n",
    " \"cont'd\": 'continued',\n",
    " \"could've\": 'could have',\n",
    " \"couldn't\": 'could not',\n",
    " \"Couldn't\": 'Could not',\n",
    " \"didn't\": 'did not',\n",
    " \"Didn't\": 'Did not',\n",
    " \"DIDN'T\": 'DID NOT',\n",
    " \"don't\": 'do not',\n",
    " \"Don't\": 'Do not',\n",
    " \"DON'T\": 'DO NOT',\n",
    " \"doesn't\": 'does not',\n",
    " \"Doesn't\": 'Does not',\n",
    " \"else's\": 'else',\n",
    " \"gov's\": 'government',\n",
    " \"Gov's\": 'government',\n",
    " \"gov't\": 'government',\n",
    " \"Gov't\": 'government',\n",
    " \"govt's\": 'government',\n",
    " \"gov'ts\": 'governments',\n",
    " \"hadn't\": 'had not',\n",
    " \"hasn't\": 'has not',\n",
    " \"Hasn't\": 'Has not',\n",
    " \"haven't\": 'have not',\n",
    " \"Haven't\": 'Have not',\n",
    " \"he's\": 'he is',\n",
    " \"He's\": 'He is',\n",
    " \"he'll\": 'he will',\n",
    " \"He'll\": 'He will',\n",
    " \"he'd\": 'he would',\n",
    " \"He'd\": 'He would',\n",
    " \"Here's\": 'Here is',\n",
    " \"here's\": 'here is',\n",
    " \"I'm\": 'I am',\n",
    " \"i'm\": 'i am',\n",
    " \"I'M\": 'I am',\n",
    " \"I've\": 'I have',\n",
    " \"i've\": 'i have',\n",
    " \"I'll\": 'I will',\n",
    " \"i'll\": 'i will',\n",
    " \"I'd\": 'I would',\n",
    " \"i'd\": 'i would',\n",
    " \"ain't\": 'is not',\n",
    " \"isn't\": 'is not',\n",
    " \"Isn't\": 'Is not',\n",
    " \"ISN'T\": 'IS NOT',\n",
    " \"it's\": 'it is',\n",
    " \"It's\": 'It is',\n",
    " \"IT'S\": 'IT IS',\n",
    " \"I's\": 'It is',\n",
    " \"i's\": 'it is',\n",
    " \"it'll\": 'it will',\n",
    " \"It'll\": 'It will',\n",
    " \"it'd\": 'it would',\n",
    " \"It'd\": 'It would',\n",
    " \"Let's\": \"Let's\",\n",
    " \"let's\": 'let us',\n",
    " \"ma'am\": 'madam',\n",
    " \"Ma'am\": \"Madam\",\n",
    " \"she's\": 'she is',\n",
    " \"She's\": 'She is',\n",
    " \"she'll\": 'she will',\n",
    " \"She'll\": 'She will',\n",
    " \"she'd\": 'she would',\n",
    " \"She'd\": 'She would',\n",
    " \"shouldn't\": 'should not',\n",
    " \"that's\": 'that is',\n",
    " \"That's\": 'That is',\n",
    " \"THAT'S\": 'THAT IS',\n",
    " \"THAT's\": 'THAT IS',\n",
    " \"that'll\": 'that will',\n",
    " \"That'll\": 'That will',\n",
    " \"there's\": 'there is',\n",
    " \"There's\": 'There is',\n",
    " \"there'll\": 'there will',\n",
    " \"There'll\": 'There will',\n",
    " \"there'd\": 'there would',\n",
    " \"they're\": 'they are',\n",
    " \"They're\": 'They are',\n",
    " \"they've\": 'they have',\n",
    " \"They've\": 'They Have',\n",
    " \"they'll\": 'they will',\n",
    " \"They'll\": 'They will',\n",
    " \"they'd\": 'they would',\n",
    " \"They'd\": 'They would',\n",
    " \"wasn't\": 'was not',\n",
    " \"we're\": 'we are',\n",
    " \"We're\": 'We are',\n",
    " \"we've\": 'we have',\n",
    " \"We've\": 'We have',\n",
    " \"we'll\": 'we will',\n",
    " \"We'll\": 'We will',\n",
    " \"we'd\": 'we would',\n",
    " \"We'd\": 'We would',\n",
    " \"What'll\": 'What will',\n",
    " \"weren't\": 'were not',\n",
    " \"Weren't\": 'Were not',\n",
    " \"what's\": 'what is',\n",
    " \"What's\": 'What is',\n",
    " \"When's\": 'When is',\n",
    " \"Where's\": 'Where is',\n",
    " \"where's\": 'where is',\n",
    " \"Where'd\": 'Where would',\n",
    " \"who're\": 'who are',\n",
    " \"who've\": 'who have',\n",
    " \"who's\": 'who is',\n",
    " \"Who's\": 'Who is',\n",
    " \"who'll\": 'who will',\n",
    " \"who'd\": 'Who would',\n",
    " \"Who'd\": 'Who would',\n",
    " \"won't\": 'will not',\n",
    " \"Won't\": 'will not',\n",
    " \"WON'T\": 'WILL NOT',\n",
    " \"would've\": 'would have',\n",
    " \"wouldn't\": 'would not',\n",
    " \"Wouldn't\": 'Would not',\n",
    " \"would't\": 'would not',\n",
    " \"Would't\": 'Would not',\n",
    " \"y'all\": 'you all',\n",
    " \"Y'all\": 'You all',\n",
    " \"you're\": 'you are',\n",
    " \"You're\": 'You are',\n",
    " \"YOU'RE\": 'YOU ARE',\n",
    " \"you've\": 'you have',\n",
    " \"You've\": 'You have',\n",
    " \"y'know\": 'you know',\n",
    " \"Y'know\": 'You know',\n",
    " \"ya'll\": 'you will',\n",
    " \"you'll\": 'you will',\n",
    " \"You'll\": 'You will',\n",
    " \"you'd\": 'you would',\n",
    " \"You'd\": 'You would',\n",
    " \"Y'got\": 'You got',\n",
    " 'cause': 'because',\n",
    " \"had'nt\": 'had not',\n",
    " \"Had'nt\": 'Had not',\n",
    " \"how'd\": 'how did',\n",
    " \"how'd'y\": 'how do you',\n",
    " \"how'll\": 'how will',\n",
    " \"how's\": 'how is',\n",
    " \"I'd've\": 'I would have',\n",
    " \"I'll've\": 'I will have',\n",
    " \"i'd've\": 'i would have',\n",
    " \"i'll've\": 'i will have',\n",
    " \"it'd've\": 'it would have',\n",
    " \"it'll've\": 'it will have',\n",
    " \"mayn't\": 'may not',\n",
    " \"might've\": 'might have',\n",
    " \"mightn't\": 'might not',\n",
    " \"mightn't've\": 'might not have',\n",
    " \"must've\": 'must have',\n",
    " \"mustn't\": 'must not',\n",
    " \"mustn't've\": 'must not have',\n",
    " \"needn't\": 'need not',\n",
    " \"needn't've\": 'need not have',\n",
    " \"o'clock\": 'of the clock',\n",
    " \"oughtn't\": 'ought not',\n",
    " \"oughtn't've\": 'ought not have',\n",
    " \"shan't\": 'shall not',\n",
    " \"sha'n't\": 'shall not',\n",
    " \"shan't've\": 'shall not have',\n",
    " \"she'd've\": 'she would have',\n",
    " \"she'll've\": 'she will have',\n",
    " \"should've\": 'should have',\n",
    " \"shouldn't've\": 'should not have',\n",
    " \"so've\": 'so have',\n",
    " \"so's\": 'so as',\n",
    " \"this's\": 'this is',\n",
    " \"that'd\": 'that would',\n",
    " \"that'd've\": 'that would have',\n",
    " \"there'd've\": 'there would have',\n",
    " \"they'd've\": 'they would have',\n",
    " \"they'll've\": 'they will have',\n",
    " \"to've\": 'to have',\n",
    " \"we'd've\": 'we would have',\n",
    " \"we'll've\": 'we will have',\n",
    " \"what'll\": 'what will',\n",
    " \"what'll've\": 'what will have',\n",
    " \"what're\": 'what are',\n",
    " \"what've\": 'what have',\n",
    " \"when's\": 'when is',\n",
    " \"when've\": 'when have',\n",
    " \"where'd\": 'where did',\n",
    " \"where've\": 'where have',\n",
    " \"who'll've\": 'who will have',\n",
    " \"why's\": 'why is',\n",
    " \"why've\": 'why have',\n",
    " \"will've\": 'will have',\n",
    " \"won't've\": 'will not have',\n",
    " \"wouldn't've\": 'would not have',\n",
    " \"y'all'd\": 'you all would',\n",
    " \"y'all'd've\": 'you all would have',\n",
    " \"y'all're\": 'you all are',\n",
    " \"y'all've\": 'you all have',\n",
    " \"you'd've\": 'you would have',\n",
    " \"you'll've\": 'you will have',\n",
    "'bebecause':'be because',\n",
    "'I’m':'I am',\n",
    "              'it’s':'it is',\n",
    "                 'I’ve':'I have',\n",
    "                 'don’t':'do not',\n",
    "                'However':'but',\n",
    "                 'It’s':'It is',\n",
    "                 'didn’t':'did not',\n",
    "                 'can’t':'can not',\n",
    "                 'that’s':'that is',\n",
    "'doesn’t':'does not',\n",
    "'I’d':'I had',\n",
    "'isn’t':'is not',\n",
    "'wasn’t':'was not'\n",
    "                \n",
    "                }\n",
    "\n",
    "def wordreplace(tweet,replaceWords):\n",
    "    for key in replaceWords:\n",
    "        tweet = tweet.replace(key,replaceWords[key])\n",
    "    return tweet\n",
    "\n",
    "for index, row in train['text'].iteritems():\n",
    "    train['text'][index] = wordreplace(row,replaceWords1)\n",
    "    \n",
    "for index, row in test['text'].iteritems():\n",
    "    test['text'][index] = wordreplace(row,replaceWords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "white_list = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "white_list += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['text'] = train['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(white_list)))\n",
    "#test['text'] = test['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(white_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks\n",
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "test['text'] = test['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespaces\n",
    "train['text'] = train['text'].apply(lambda x:' '.join(x.split()))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy's language model\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# function to lemmatize text\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = lemmatization(train['text'])\n",
    "test['text'] = lemmatization(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['text']\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train , x_test, y_train  , y_test = train_test_split(X , \n",
    "                                                     y , \n",
    "                                                      stratify = y.values , \n",
    "                                                     train_size = 0.8,\n",
    "                                                     random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /kaggle into sys.path.\n"
     ]
    }
   ],
   "source": [
    "tfhub_dir = '/data/jupyter/common/model/text/tfhub'\n",
    "\n",
    "import sys, os\n",
    "def add_aion(curr_path=None):\n",
    "    if curr_path is None:\n",
    "        dir_path = os.getcwd()\n",
    "        target_path = os.path.dirname(dir_path)\n",
    "        if target_path not in sys.path:\n",
    "            print('Added %s into sys.path.' % (target_path))\n",
    "            sys.path.insert(0, target_path)\n",
    "            \n",
    "add_aion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 43111\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for sentence in x_train:\n",
    "    tokens = sentence.split(' ')\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('Vocab Size: %d' % (vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 270\n",
    "word2Idx = {'<padding>': 0, '<unknown>': 1}\n",
    "idx2word = {0: '<padding>', 1: '<unknown>'}\n",
    "\n",
    "def preprocess(text, word2Idx, idx2word, training=False):\n",
    "    if training:\n",
    "        for sentence in text:\n",
    "            tokens = sentence.split(' ')\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in word2Idx:\n",
    "                    word2Idx[token] = len(word2Idx)\n",
    "                    idx2word[len(word2Idx)-1] = token\n",
    "\n",
    "\n",
    "    word_vectors = np.zeros((len(text), max_sentence_length))\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for i, sentence in enumerate(text):\n",
    "        ids = []\n",
    "        words = []\n",
    "        tokens = sentence.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in word2Idx:\n",
    "                ids.append(word2Idx[token])\n",
    "                words.append(token)\n",
    "            else:\n",
    "                ids.append(word2Idx['<unknown>'])\n",
    "                words.append('<unknown>')\n",
    "                \n",
    "            if len(ids) >= max_sentence_length:\n",
    "                break\n",
    "\n",
    "        for i in range(max_sentence_length - len(ids)):\n",
    "            ids.append(word2Idx['<padding>'])\n",
    "            words.append('<padding>')\n",
    "\n",
    "        word_vectors[i] = np.asarray(ids)\n",
    "        sentence_vectors.append(' '.join(words))\n",
    "\n",
    "    sentence_vectors = np.asarray(sentence_vectors)\n",
    "    \n",
    "    return word2Idx, idx2word, word_vectors, sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "class ELMoEmbedding(Layer):\n",
    "\n",
    "    def __init__(self, idx2word, output_mode=\"default\", trainable=True, **kwargs):\n",
    "        assert output_mode in [\"default\", \"word_emb\", \"lstm_outputs1\", \"lstm_outputs2\", \"elmo\"]\n",
    "        assert trainable in [True, False]\n",
    "        self.idx2word = idx2word\n",
    "        self.output_mode = output_mode\n",
    "        self.trainable = trainable\n",
    "        self.max_length = None\n",
    "        self.word_mapping = None\n",
    "        self.lookup_table = None\n",
    "        self.elmo_model = None\n",
    "        self.embedding = None\n",
    "        super(ELMoEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.max_length = input_shape[1]\n",
    "        self.word_mapping = [x[1] for x in sorted(self.idx2word.items(), key=lambda x: x[0])]\n",
    "        self.lookup_table = tf.contrib.lookup.index_to_string_table_from_tensor(self.word_mapping, default_value=\"<UNK>\")\n",
    "        self.lookup_table.init.run(session=K.get_session())\n",
    "        self.elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=self.trainable)\n",
    "        super(ELMoEmbedding, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, dtype=tf.int64)\n",
    "        sequence_lengths = tf.cast(tf.count_nonzero(x, axis=1), dtype=tf.int32)\n",
    "        strings = self.lookup_table.lookup(x)\n",
    "        inputs = {\n",
    "            \"tokens\": strings,\n",
    "            \"sequence_len\": sequence_lengths\n",
    "        }\n",
    "        return self.elmo_model(inputs, signature=\"tokens\", as_dict=True)[self.output_mode]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.output_mode == \"default\":\n",
    "            return (input_shape[0], 1024)\n",
    "        if self.output_mode == \"word_emb\":\n",
    "            return (input_shape[0], self.max_length, 512)\n",
    "        if self.output_mode == \"lstm_outputs1\":\n",
    "            return (input_shape[0], self.max_length, 1024)\n",
    "        if self.output_mode == \"lstm_outputs2\":\n",
    "            return (input_shape[0], self.max_length, 1024)\n",
    "        if self.output_mode == \"elmo\":\n",
    "            return (input_shape[0], self.max_length, 1024)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'idx2word': self.idx2word,\n",
    "            'output_mode': self.output_mode \n",
    "        }\n",
    "        return list(config.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_words.shape: (4223, 270)\n",
      "x_test_words.shape: (1056, 270)\n"
     ]
    }
   ],
   "source": [
    "word2Idx, idx2word, x_train_words, x_train_sentences = preprocess(\n",
    "    text=x_train, word2Idx=word2Idx, idx2word=idx2word, training=True)\n",
    "print('x_train_words.shape:', x_train_words.shape)\n",
    "\n",
    "word2Idx, idx2word, x_test_words, x_test_sentences = preprocess(\n",
    "    text=x_test, word2Idx=word2Idx, idx2word=idx2word, training=False)\n",
    "print('x_test_words.shape:', x_test_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted sum of the 3 layers with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Dense, Embedding, BatchNormalization, Concatenate, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, urllib, zipfile\n",
    "\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, verbose=0):\n",
    "        self.verbose = verbose\n",
    "        self.model = {}\n",
    "        self.model_path = ''\n",
    "        \n",
    "    def _log_time(self, status, msg, verbose):\n",
    "        if self.verbose >= 10 or verbose >= 10:\n",
    "            print('%s. [%s] %s' % (datetime.datetime.now(), status, msg))\n",
    "            \n",
    "    def download(self, src, dest_dir, dest_file, uncompress, housekeep=False, verbose=0):\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "    \n",
    "        if dest_file is None:\n",
    "            dest_file = os.path.basename(src)\n",
    "            \n",
    "        if not self.is_file_exist(dest_dir + dest_file):\n",
    "            self._log_time(status='DOWNLOAD', msg='From '+src+' to '+dest_dir+dest_file, verbose=verbose)\n",
    "            file = urllib.request.urlopen(src)\n",
    "            with open(dest_dir + dest_file,'wb') as output:\n",
    "                output.write(file.read())\n",
    "        else:\n",
    "            self._log_time(status='FOUND', msg=dest_file+' in '+dest_dir, verbose=verbose)\n",
    "            \n",
    "        if uncompress:\n",
    "            self.uncompress(dest_dir + dest_file)\n",
    "            \n",
    "        if uncompress and housekeep:\n",
    "            self.housekeep(dest_dir + dest_file)\n",
    "            \n",
    "            \n",
    "        return dest_dir + dest_file\n",
    "    \n",
    "    \"\"\"\n",
    "        File related\n",
    "    \"\"\"\n",
    "    \n",
    "    def uncompress(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def unzip(self, file_path):\n",
    "        dest_dir = os.path.dirname(file_path)\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "            \n",
    "    def housekeep(self, file_path):\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    def is_file_exist(self, file_path):\n",
    "        return os.path.exists(file_path)\n",
    "    \n",
    "    def save(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def load(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    \"\"\"\n",
    "        Model related\n",
    "    \"\"\"\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def load(self, src=None, dest_dir=None, trainable=True, verbose=0):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    \"\"\"\n",
    "        Vocabulary realted\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_vocab(self, **kwargs):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def _tokenizer_space(self, sentence):\n",
    "        return sentence.split(' ')\n",
    "        \n",
    "    \"\"\"\n",
    "        Vector related\n",
    "    \"\"\"\n",
    "    \n",
    "    def train(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def encode(self, sentences):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def visualize(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    \"\"\"\n",
    "        Netowrk realted\n",
    "    \"\"\"\n",
    "    \n",
    "    def to_numpy_layer(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def to_keras_layer(self):\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def to_tensorflow_layer(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def to_pytorch_layer(self):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddings(Embeddings):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 handle_oov=True, oov_vector=None, oov_vector_type='zero',\n",
    "                 padding=True, pad_vector=None, pad_vector_type='zero',\n",
    "                 max_sequence_length=350, dimension=350,\n",
    "                 verbose=0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.handle_oov = handle_oov\n",
    "        self.oov_vector_type = oov_vector_type\n",
    "        if handle_oov and oov_vector is None:\n",
    "            if oov_vector_type == 'zero':\n",
    "                self.oov_vector = np.zeros(dimension)\n",
    "            elif oov_vector_type == 'random':\n",
    "                self.oov_vector = np.random.rand(dimension)\n",
    "        else:\n",
    "            self.oov_vector = oov_vector\n",
    "            \n",
    "        self.padding = padding\n",
    "        self.pad_vector_type = pad_vector_type\n",
    "        if padding and pad_vector is None:\n",
    "            if pad_vector_type == 'zero':\n",
    "                self.pad_vector = np.zeros(dimension)\n",
    "            elif pad_vector_type == 'random':\n",
    "                self.pad_vector = np.random.rand(dimension)\n",
    "        else:\n",
    "            self.pad_vector = pad_vector\n",
    "        \n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.dimension = dimension\n",
    "        \n",
    "    def get_oov_vector(self):\n",
    "        return self.oov_vector\n",
    "        \n",
    "    def set_oov_vector(self, oov_vector):\n",
    "        self.oov_vector = oov_vector\n",
    "        \n",
    "    def get_pad_vector(self):\n",
    "        return self.pad_vector\n",
    "        \n",
    "    def set_pad_vector(self, pad_vector):\n",
    "        self.pad_vector = pad_vector\n",
    "        \n",
    "    def is_vector_exist(self, word):\n",
    "        return word in self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "\n",
    "\n",
    "class ELMoEmbeddings(WordEmbeddings):\n",
    "    ELMO_MODEL_V2_URL = \"https://tfhub.dev/google/elmo/2\"\n",
    "\n",
    "    def __init__(self, layer, verbose=0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.layer = layer\n",
    "        \n",
    "    def _set_tf_log_level(self, verbose):\n",
    "        if verbose >= 30:\n",
    "            tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        elif verbose >= 20:\n",
    "            tf.logging.set_verbosity(tf.logging.WARN)\n",
    "        elif verbose >= 10:\n",
    "            tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "        else:\n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        \n",
    "    def load(self, src=None, dest_dir=None, trainable=True, verbose=0):\n",
    "        self._log_time(status='LOADING', msg='file', verbose=verbose)\n",
    "        self._set_tf_log_level(verbose)\n",
    "        \n",
    "        if src == None:\n",
    "            src = self.ELMO_MODEL_V2_URL\n",
    "        \n",
    "        if dest_dir is not None:\n",
    "            os.environ[\"TFHUB_CACHE_DIR\"] = dest_dir\n",
    "        \n",
    "        self.model = tf_hub.Module(src, trainable=trainable)\n",
    "        \n",
    "        self._log_time(status='LOADED', msg='', verbose=verbose)\n",
    "        \n",
    "        return self.model    \n",
    "    \n",
    "    def to_keras_layer(self, x):\n",
    "        # Source: https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb\n",
    "        '''\n",
    "            For signature and layer parameters, you can visit https://alpha.tfhub.dev/google/elmo/2\n",
    "        '''        \n",
    "        return self.model(\n",
    "            tf.squeeze(tf.cast(x, tf.string)), \n",
    "            signature=\"default\", as_dict=True)[self.layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-28 16:18:34.664078. [LOADING] file\n",
      "2019-07-28 16:18:55.597850. [LOADED] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x7f620f0f48d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embs = ELMoEmbeddings(layer='elmo', verbose=20)\n",
    "elmo_embs.load(dest_dir=tfhub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "es = EarlyStopping(monitor='val_loss', mode ='min' ,verbose =1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    11036416    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1024)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 1280)   0           embedding_1[0][0]                \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 1280)   5120        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 256)    1573888     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          25700       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            303         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,166,739\n",
      "Trainable params: 13,164,179\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "Train on 4223 samples, validate on 1056 samples\n",
      "Epoch 1/100\n",
      "4223/4223 [==============================] - 155s 37ms/step - loss: 0.7985 - acc: 0.7116 - val_loss: 0.7758 - val_acc: 0.7244\n",
      "Epoch 2/100\n",
      "4223/4223 [==============================] - 143s 34ms/step - loss: 0.7702 - acc: 0.7246 - val_loss: 0.7808 - val_acc: 0.7244\n",
      "Epoch 3/100\n",
      "4223/4223 [==============================] - 143s 34ms/step - loss: 0.7512 - acc: 0.7246 - val_loss: 0.7833 - val_acc: 0.7244\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61fbc12e10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Layers\n",
    "word_input_layer = Input(shape=(None, ), dtype='int32')\n",
    "elmo_input_layer = Input(shape=(None, ), dtype=tf.string)\n",
    "\n",
    "# Output Layers\n",
    "word_output_layer = Embedding(\n",
    "    input_dim=vocab_size, output_dim=256)(word_input_layer)\n",
    "elmo_output_layer = Lambda(\n",
    "    elmo_embs.to_keras_layer, \n",
    "    output_shape=(None, 1024))(elmo_input_layer)\n",
    "output_layer = Concatenate()(\n",
    "    [word_output_layer, elmo_output_layer])\n",
    "output_layer = BatchNormalization()(output_layer)\n",
    "output_layer = LSTM(\n",
    "    256, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)(output_layer)\n",
    "output_layer = LSTM(\n",
    "    256, dropout=0.2, recurrent_dropout=0.2)(output_layer)\n",
    "output_layer = Dense(100, activation='relu')(output_layer)\n",
    "output_layer = Dense(3, activation='softmax')(output_layer)\n",
    "\n",
    "# Build Model\n",
    "model = Model(\n",
    "    inputs=[word_input_layer, elmo_input_layer], \n",
    "    outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(\n",
    "    [x_train_words, x_train_sentences], y_train,\n",
    "     validation_data=([x_test_words, x_test_sentences], y_test), \n",
    "     callbacks=[es],\n",
    "    class_weight=class_weights,\n",
    "    epochs=100, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_test_words, x_test_sentences])\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       123\n",
      "           1       0.00      0.00      0.00       168\n",
      "           2       0.72      1.00      0.84       765\n",
      "\n",
      "    accuracy                           0.72      1056\n",
      "   macro avg       0.24      0.33      0.28      1056\n",
      "weighted avg       0.52      0.72      0.61      1056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_words.shape: (2924, 270)\n"
     ]
    }
   ],
   "source": [
    "word2Idx, idx2word, test_words, test_sentences = preprocess(\n",
    "    text=test['text'], word2Idx=word2Idx, idx2word=idx2word, training=True)\n",
    "print('test_words.shape:', test_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict([test_words,test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_round = np.argmax(predictions,axis=1)\n",
    "sub = pd.read_csv(\"../input/sample_submission_i5xnIZD.csv\")\n",
    "sub['sentiment'] = prediction_round\n",
    "sub.to_csv(\"ELMO_1L.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted sum of the 3 layers without word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-28 16:28:20.516427. [LOADING] file\n",
      "2019-07-28 16:28:21.395307. [LOADED] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x7f61e4579320>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Dense, Embedding, BatchNormalization, Concatenate, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "elmo_embs = ELMoEmbeddings(layer='elmo', verbose=20)\n",
    "elmo_embs.load(dest_dir=tfhub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               1311744   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,316,611\n",
      "Trainable params: 1,314,563\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Train on 4223 samples, validate on 1056 samples\n",
      "Epoch 1/10\n",
      "4223/4223 [==============================] - 130s 31ms/step - loss: 0.8393 - acc: 0.7080 - val_loss: 0.7759 - val_acc: 0.7244\n",
      "Epoch 2/10\n",
      "4223/4223 [==============================] - 127s 30ms/step - loss: 0.7383 - acc: 0.7272 - val_loss: 0.7972 - val_acc: 0.7216\n",
      "Epoch 3/10\n",
      "4223/4223 [==============================] - 126s 30ms/step - loss: 0.7129 - acc: 0.7338 - val_loss: 0.7971 - val_acc: 0.7169\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5dcc9d1cc0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Layers\n",
    "elmo_input_layer = Input(shape=(None, ), dtype=tf.string)\n",
    "\n",
    "# Output Layers\n",
    "output_layer = Lambda(\n",
    "    elmo_embs.to_keras_layer, \n",
    "    output_shape=(None, 1024))(elmo_input_layer)\n",
    "output_layer = BatchNormalization()(output_layer)\n",
    "output_layer = LSTM(\n",
    "    256, dropout=0.2, recurrent_dropout=0.2)(output_layer)\n",
    "output_layer = Dense(3, activation='softmax')(output_layer)\n",
    "\n",
    "# Build Model\n",
    "model = Model(\n",
    "    inputs=elmo_input_layer, outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(\n",
    "    x_train_sentences, y_train,\n",
    "    validation_data=(x_test_sentences, y_test), \n",
    "    callbacks=[es],\n",
    "    class_weight=class_weights,\n",
    "    epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_sentences)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       123\n",
      "           1       0.50      0.01      0.02       168\n",
      "           2       0.72      0.99      0.84       765\n",
      "\n",
      "    accuracy                           0.72      1056\n",
      "   macro avg       0.41      0.33      0.29      1056\n",
      "weighted avg       0.60      0.72      0.61      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_round = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/sample_submission_i5xnIZD.csv\")\n",
    "sub['sentiment'] = prediction_round\n",
    "sub.to_csv(\"ELMO_2L.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Mean-pooling without word embeddings¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-28 16:36:38.322531. [LOADING] file\n",
      "2019-07-28 16:36:39.100433. [LOADED] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x7f5dcbdb5da0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Dense, Embedding, BatchNormalization, Concatenate, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "elmo_embs = ELMoEmbeddings(layer='default', verbose=20)\n",
    "elmo_embs.load(dest_dir=tfhub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 263,171\n",
      "Trainable params: 263,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4223 samples, validate on 1056 samples\n",
      "Epoch 1/10\n",
      "4223/4223 [==============================] - 138s 33ms/step - loss: 0.7913 - acc: 0.7156 - val_loss: 0.7656 - val_acc: 0.7244\n",
      "Epoch 2/10\n",
      "4223/4223 [==============================] - 135s 32ms/step - loss: 0.7557 - acc: 0.7220 - val_loss: 0.7774 - val_acc: 0.7263\n",
      "Epoch 3/10\n",
      "4223/4223 [==============================] - 135s 32ms/step - loss: 0.7413 - acc: 0.7241 - val_loss: 0.7968 - val_acc: 0.7235\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5dc993a9b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Layers\n",
    "input_layer = Input(shape=(None,), dtype=tf.string)\n",
    "\n",
    "# Output Layers\n",
    "output_layer = Lambda(\n",
    "    elmo_embs.to_keras_layer, \n",
    "    output_shape=(1024,))(input_layer)\n",
    "output_layer = Dense(\n",
    "    256, activation='relu')(output_layer)\n",
    "output_layer = Dense(3, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "model.fit(\n",
    "    x_train_sentences, y_train,\n",
    "    validation_data=(x_test_sentences, y_test), \n",
    "    callbacks=[es],\n",
    "    epochs=10, batch_size=32,class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_sentences)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.01      0.02       123\n",
      "           1       0.00      0.00      0.00       168\n",
      "           2       0.73      1.00      0.84       765\n",
      "\n",
      "    accuracy                           0.72      1056\n",
      "   macro avg       0.33      0.34      0.29      1056\n",
      "weighted avg       0.55      0.72      0.61      1056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(test_sentences)\n",
    "prediction_round = np.argmax(predictions,axis=1)\n",
    "sub = pd.read_csv(\"../input/sample_submission_i5xnIZD.csv\")\n",
    "sub['sentiment'] = prediction_round\n",
    "sub.to_csv(\"ELMO_3L.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
