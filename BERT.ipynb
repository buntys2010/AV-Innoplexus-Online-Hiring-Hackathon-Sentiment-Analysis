{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_tOlRoBf.csv', 'train_F3WbcTw.csv', 'sample_submission_i5xnIZD.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import fileinput\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import datetime\n",
    "import sys\n",
    "from tqdm  import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_F3WbcTw.csv\")\n",
    "test = pd.read_csv(\"../input/test_tOlRoBf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'] + \"This observation is for \" + train['drug']\n",
    "test['text'] = test['text'] + \"This observation is for \" + test['drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    new_text =re.sub('[0-9]', '', text)\n",
    "    new_text = re.sub(r\"\\u200b\",\"\",new_text)\n",
    "    new_text = re.sub(r\"\\.+\",\".\",new_text)\n",
    "    new_text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',new_text, flags=re.MULTILINE)\n",
    "    new_text = re.sub(\"'\", \"\", new_text)\n",
    "    new_text = re.sub(r'â†‘', '', new_text)\n",
    "    new_text = re.sub(\"\\t\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\xa0\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\(|\\)|\\[|\\]\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\n\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\.\", \"\", new_text)\n",
    "    new_text = re.sub(\"\\,\", \" \", new_text)\n",
    "    new_text = re.sub(\"[/%]\", \" \", new_text)\n",
    "    new_text = re.sub('[/%:;]', '', new_text)\n",
    "    new_text = re.sub(' +', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 54s, sys: 128 ms, total: 6min 54s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove URL's from train and test\n",
    "for index, row in train['text'].iteritems():\n",
    "    train['text'][index] = pre_process(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 20 ms, total: 2.22 s\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for index, row in test['text'].iteritems():\n",
    "    test['text'][index] = pre_process(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Won't \n",
    "\n",
    "replaceWords1 = { \"won't\":\"will not\",\"$&@*#\":\"in most profane vulgar shitty terms\",\"#$&@*#\":\"shitty\",\n",
    " \"can't\":\"cannot\",\"aren't\": 'are not',\n",
    " \"Aren't\": 'Are not',\n",
    " \"AREN'T\": 'ARE NOT',\n",
    " \"C'est\": \"C'est\",\n",
    " \"C'mon\": \"C'mon\",\n",
    " \"c'mon\": \"c'mon\",\n",
    " \"can't\": 'cannot',\n",
    " \"Can't\": 'Cannot',\n",
    " \"CAN'T\": 'CANNOT',\n",
    " \"con't\": 'continued',\n",
    " \"cont'd\": 'continued',\n",
    " \"could've\": 'could have',\n",
    " \"couldn't\": 'could not',\n",
    " \"Couldn't\": 'Could not',\n",
    " \"didn't\": 'did not',\n",
    " \"Didn't\": 'Did not',\n",
    " \"DIDN'T\": 'DID NOT',\n",
    " \"don't\": 'do not',\n",
    " \"Don't\": 'Do not',\n",
    " \"DON'T\": 'DO NOT',\n",
    " \"doesn't\": 'does not',\n",
    " \"Doesn't\": 'Does not',\n",
    " \"else's\": 'else',\n",
    " \"gov's\": 'government',\n",
    " \"Gov's\": 'government',\n",
    " \"gov't\": 'government',\n",
    " \"Gov't\": 'government',\n",
    " \"govt's\": 'government',\n",
    " \"gov'ts\": 'governments',\n",
    " \"hadn't\": 'had not',\n",
    " \"hasn't\": 'has not',\n",
    " \"Hasn't\": 'Has not',\n",
    " \"haven't\": 'have not',\n",
    " \"Haven't\": 'Have not',\n",
    " \"he's\": 'he is',\n",
    " \"He's\": 'He is',\n",
    " \"he'll\": 'he will',\n",
    " \"He'll\": 'He will',\n",
    " \"he'd\": 'he would',\n",
    " \"He'd\": 'He would',\n",
    " \"Here's\": 'Here is',\n",
    " \"here's\": 'here is',\n",
    " \"I'm\": 'I am',\n",
    " \"i'm\": 'i am',\n",
    " \"I'M\": 'I am',\n",
    " \"I've\": 'I have',\n",
    " \"i've\": 'i have',\n",
    " \"I'll\": 'I will',\n",
    " \"i'll\": 'i will',\n",
    " \"I'd\": 'I would',\n",
    " \"i'd\": 'i would',\n",
    " \"ain't\": 'is not',\n",
    " \"isn't\": 'is not',\n",
    " \"Isn't\": 'Is not',\n",
    " \"ISN'T\": 'IS NOT',\n",
    " \"it's\": 'it is',\n",
    " \"It's\": 'It is',\n",
    " \"IT'S\": 'IT IS',\n",
    " \"I's\": 'It is',\n",
    " \"i's\": 'it is',\n",
    " \"it'll\": 'it will',\n",
    " \"It'll\": 'It will',\n",
    " \"it'd\": 'it would',\n",
    " \"It'd\": 'It would',\n",
    " \"Let's\": \"Let's\",\n",
    " \"let's\": 'let us',\n",
    " \"ma'am\": 'madam',\n",
    " \"Ma'am\": \"Madam\",\n",
    " \"she's\": 'she is',\n",
    " \"She's\": 'She is',\n",
    " \"she'll\": 'she will',\n",
    " \"She'll\": 'She will',\n",
    " \"she'd\": 'she would',\n",
    " \"She'd\": 'She would',\n",
    " \"shouldn't\": 'should not',\n",
    " \"that's\": 'that is',\n",
    " \"That's\": 'That is',\n",
    " \"THAT'S\": 'THAT IS',\n",
    " \"THAT's\": 'THAT IS',\n",
    " \"that'll\": 'that will',\n",
    " \"That'll\": 'That will',\n",
    " \"there's\": 'there is',\n",
    " \"There's\": 'There is',\n",
    " \"there'll\": 'there will',\n",
    " \"There'll\": 'There will',\n",
    " \"there'd\": 'there would',\n",
    " \"they're\": 'they are',\n",
    " \"They're\": 'They are',\n",
    " \"they've\": 'they have',\n",
    " \"They've\": 'They Have',\n",
    " \"they'll\": 'they will',\n",
    " \"They'll\": 'They will',\n",
    " \"they'd\": 'they would',\n",
    " \"They'd\": 'They would',\n",
    " \"wasn't\": 'was not',\n",
    " \"we're\": 'we are',\n",
    " \"We're\": 'We are',\n",
    " \"we've\": 'we have',\n",
    " \"We've\": 'We have',\n",
    " \"we'll\": 'we will',\n",
    " \"We'll\": 'We will',\n",
    " \"we'd\": 'we would',\n",
    " \"We'd\": 'We would',\n",
    " \"What'll\": 'What will',\n",
    " \"weren't\": 'were not',\n",
    " \"Weren't\": 'Were not',\n",
    " \"what's\": 'what is',\n",
    " \"What's\": 'What is',\n",
    " \"When's\": 'When is',\n",
    " \"Where's\": 'Where is',\n",
    " \"where's\": 'where is',\n",
    " \"Where'd\": 'Where would',\n",
    " \"who're\": 'who are',\n",
    " \"who've\": 'who have',\n",
    " \"who's\": 'who is',\n",
    " \"Who's\": 'Who is',\n",
    " \"who'll\": 'who will',\n",
    " \"who'd\": 'Who would',\n",
    " \"Who'd\": 'Who would',\n",
    " \"won't\": 'will not',\n",
    " \"Won't\": 'will not',\n",
    " \"WON'T\": 'WILL NOT',\n",
    " \"would've\": 'would have',\n",
    " \"wouldn't\": 'would not',\n",
    " \"Wouldn't\": 'Would not',\n",
    " \"would't\": 'would not',\n",
    " \"Would't\": 'Would not',\n",
    " \"y'all\": 'you all',\n",
    " \"Y'all\": 'You all',\n",
    " \"you're\": 'you are',\n",
    " \"You're\": 'You are',\n",
    " \"YOU'RE\": 'YOU ARE',\n",
    " \"you've\": 'you have',\n",
    " \"You've\": 'You have',\n",
    " \"y'know\": 'you know',\n",
    " \"Y'know\": 'You know',\n",
    " \"ya'll\": 'you will',\n",
    " \"you'll\": 'you will',\n",
    " \"You'll\": 'You will',\n",
    " \"you'd\": 'you would',\n",
    " \"You'd\": 'You would',\n",
    " \"Y'got\": 'You got',\n",
    " 'cause': 'because',\n",
    " \"had'nt\": 'had not',\n",
    " \"Had'nt\": 'Had not',\n",
    " \"how'd\": 'how did',\n",
    " \"how'd'y\": 'how do you',\n",
    " \"how'll\": 'how will',\n",
    " \"how's\": 'how is',\n",
    " \"I'd've\": 'I would have',\n",
    " \"I'll've\": 'I will have',\n",
    " \"i'd've\": 'i would have',\n",
    " \"i'll've\": 'i will have',\n",
    " \"it'd've\": 'it would have',\n",
    " \"it'll've\": 'it will have',\n",
    " \"mayn't\": 'may not',\n",
    " \"might've\": 'might have',\n",
    " \"mightn't\": 'might not',\n",
    " \"mightn't've\": 'might not have',\n",
    " \"must've\": 'must have',\n",
    " \"mustn't\": 'must not',\n",
    " \"mustn't've\": 'must not have',\n",
    " \"needn't\": 'need not',\n",
    " \"needn't've\": 'need not have',\n",
    " \"o'clock\": 'of the clock',\n",
    " \"oughtn't\": 'ought not',\n",
    " \"oughtn't've\": 'ought not have',\n",
    " \"shan't\": 'shall not',\n",
    " \"sha'n't\": 'shall not',\n",
    " \"shan't've\": 'shall not have',\n",
    " \"she'd've\": 'she would have',\n",
    " \"she'll've\": 'she will have',\n",
    " \"should've\": 'should have',\n",
    " \"shouldn't've\": 'should not have',\n",
    " \"so've\": 'so have',\n",
    " \"so's\": 'so as',\n",
    " \"this's\": 'this is',\n",
    " \"that'd\": 'that would',\n",
    " \"that'd've\": 'that would have',\n",
    " \"there'd've\": 'there would have',\n",
    " \"they'd've\": 'they would have',\n",
    " \"they'll've\": 'they will have',\n",
    " \"to've\": 'to have',\n",
    " \"we'd've\": 'we would have',\n",
    " \"we'll've\": 'we will have',\n",
    " \"what'll\": 'what will',\n",
    " \"what'll've\": 'what will have',\n",
    " \"what're\": 'what are',\n",
    " \"what've\": 'what have',\n",
    " \"when's\": 'when is',\n",
    " \"when've\": 'when have',\n",
    " \"where'd\": 'where did',\n",
    " \"where've\": 'where have',\n",
    " \"who'll've\": 'who will have',\n",
    " \"why's\": 'why is',\n",
    " \"why've\": 'why have',\n",
    " \"will've\": 'will have',\n",
    " \"won't've\": 'will not have',\n",
    " \"wouldn't've\": 'would not have',\n",
    " \"y'all'd\": 'you all would',\n",
    " \"y'all'd've\": 'you all would have',\n",
    " \"y'all're\": 'you all are',\n",
    " \"y'all've\": 'you all have',\n",
    " \"you'd've\": 'you would have',\n",
    " \"you'll've\": 'you will have',\n",
    "'bebecause':'be because',\n",
    "'Iâ€™m':'I am',\n",
    "              'itâ€™s':'it is',\n",
    "                 'Iâ€™ve':'I have',\n",
    "                 'donâ€™t':'do not',\n",
    "                'However':'but',\n",
    "                 'Itâ€™s':'It is',\n",
    "                 'didnâ€™t':'did not',\n",
    "                 'canâ€™t':'can not',\n",
    "                 'thatâ€™s':'that is',\n",
    "'doesnâ€™t':'does not',\n",
    "'Iâ€™d':'I had',\n",
    "'isnâ€™t':'is not',\n",
    "'wasnâ€™t':'was not'\n",
    "                \n",
    "                }\n",
    "\n",
    "def wordreplace(tweet,replaceWords):\n",
    "    for key in replaceWords:\n",
    "        tweet = tweet.replace(key,replaceWords[key])\n",
    "    return tweet\n",
    "\n",
    "#for index, row in train['text'].iteritems():\n",
    "#    train['text'][index] = wordreplace(row,replaceWords1)\n",
    "    \n",
    "#for index, row in test['text'].iteritems():\n",
    "#    test['text'][index] = wordreplace(row,replaceWords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-28 18:00:22--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 407727028 (389M) [application/zip]\r\n",
      "Saving to: â€˜uncased_L-12_H-768_A-12.zipâ€™\r\n",
      "\r\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M   127MB/s    in 3.2s    \r\n",
      "\r\n",
      "2019-07-28 18:00:26 (120 MB/s) - â€˜uncased_L-12_H-768_A-12.zipâ€™ saved [407727028/407727028]\r\n",
      "\r\n",
      "--2019-07-28 18:00:26--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 37922 (37K) [text/plain]\r\n",
      "Saving to: â€˜modeling.pyâ€™\r\n",
      "\r\n",
      "modeling.py         100%[===================>]  37.03K  --.-KB/s    in 0.01s   \r\n",
      "\r\n",
      "2019-07-28 18:00:27 (2.87 MB/s) - â€˜modeling.pyâ€™ saved [37922/37922]\r\n",
      "\r\n",
      "--2019-07-28 18:00:27--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 6258 (6.1K) [text/plain]\r\n",
      "Saving to: â€˜optimization.pyâ€™\r\n",
      "\r\n",
      "optimization.py     100%[===================>]   6.11K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2019-07-28 18:00:27 (40.0 MB/s) - â€˜optimization.pyâ€™ saved [6258/6258]\r\n",
      "\r\n",
      "--2019-07-28 18:00:28--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 34783 (34K) [text/plain]\r\n",
      "Saving to: â€˜run_classifier.pyâ€™\r\n",
      "\r\n",
      "run_classifier.py   100%[===================>]  33.97K  --.-KB/s    in 0.01s   \r\n",
      "\r\n",
      "2019-07-28 18:00:28 (2.66 MB/s) - â€˜run_classifier.pyâ€™ saved [34783/34783]\r\n",
      "\r\n",
      "--2019-07-28 18:00:29--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12257 (12K) [text/plain]\r\n",
      "Saving to: â€˜tokenization.pyâ€™\r\n",
      "\r\n",
      "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2019-07-28 18:00:29 (64.4 MB/s) - â€˜tokenization.pyâ€™ saved [12257/12257]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'model_folder'\n",
    "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Model output directory: model_folder/outputs\n",
      ">>  BERT pretrained directory: model_folder/uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
    "OUTPUT_DIR = f'{folder}/outputs'\n",
    "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
    "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['text']\n",
    "y = train['sentiment']\n",
    "test_pred = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify = y.values , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type, labels=None):\n",
    "#Generate data for the BERT model\n",
    "    guid = f'{set_type}'\n",
    "    examples = []\n",
    "    if guid == 'train':\n",
    "        for line, label in zip(lines, labels):\n",
    "            text_a = line\n",
    "            label = str(label)\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            text_a = line\n",
    "            label = '0'\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "EVAL_BATCH_SIZE = 20\n",
    "LEARNING_RATE = 1e-5 #ncreasing the LR\n",
    "NUM_TRAIN_EPOCHS = 35.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
    "# each checpoint weights about 1,5gb\n",
    "ITERATIONS_PER_LOOP = 100000\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
    "\n",
    "label_list = [str(num) for num in range(8)]\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
    "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
    "\n",
    "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
    "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = run_classifier.model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
    "    use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      ">> Started training at 2019-07-28 18:01:14.635404 \n",
      "  Num examples = 4223\n",
      "  Batch size = 30\n",
      ">> Finished training at 2019-07-28 19:07:21.181855\n"
     ]
    }
   ],
   "source": [
    "print('Please wait...')\n",
    "train_features = run_classifier.convert_examples_to_features(\n",
    "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "print('>> Started training at {} '.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('>> Finished training at {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    print(params)\n",
    "    batch_size = 500\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_examples = create_examples(X_test, 'test')\n",
    "\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for prediction in result:\n",
    "      preds.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BERT is: 0.6979166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.41      0.38       123\n",
      "           1       0.47      0.42      0.45       168\n",
      "           2       0.81      0.80      0.81       765\n",
      "\n",
      "    accuracy                           0.70      1056\n",
      "   macro avg       0.54      0.55      0.54      1056\n",
      "weighted avg       0.70      0.70      0.70      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_examples = create_examples(test['text'], 'test')\n",
    "\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "preds_test = []\n",
    "for prediction in result:\n",
    "      preds_test.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/sample_submission_i5xnIZD.csv\")\n",
    "sub['sentiment'] = preds_test\n",
    "sub.to_csv(\"ELMO_1L.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = []\n",
    "for prediction in result:\n",
    "      preds_test.append(prediction['probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_BERT = pd.DataFrame(preds_test)\n",
    "avg_BERT.to_csv(\"BERT_prob.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
